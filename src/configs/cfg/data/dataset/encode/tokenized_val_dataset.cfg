# train_dataset args

# streamingdataset args
--data.train_dataset.local=data/mds_tokenized/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/val
# --data.train_dataset.batch_size=2
--data.train_dataset.shuffle=True
# --data.train_dataset.epoch_size=1000

# streamingdataset custom args
--data.train_dataset.group_idx_name_mapping_path=data/mds_tokenized/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/val/group_idx_name_mapping.json

# data_preprocessor args
--data.train_dataset.gene_expr_df_path=data/processed/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/gene_expr.filtered.parquet
--data.train_dataset.sample_idx_path=data/processed/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/sample_tissue_count_with_idx.csv
--data.train_dataset.num_nbase=1000
--data.train_dataset.gene_expr_quantization=True
--data.train_dataset.is_sequence_tokenized=True
# --data.train_dataset.num_gene_expr_bins=51
# --data.train_dataset.dna_tokenizer_name=zhihan1996/DNABERT-2-117M


# val_dataset args
# streamingdataset args
--data.val_dataset.local=data/mds_tokenized/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/val
# --data.val_dataset.batch_size=2
--data.val_dataset.shuffle=False
# --data.val_dataset.epoch_size=1000

# streamingdataset custom args
--data.val_dataset.group_idx_name_mapping_path=data/mds_tokenized/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/val/group_idx_name_mapping.json

# data_preprocessor args
--data.val_dataset.gene_expr_df_path=data/processed/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/gene_expr.filtered.parquet
--data.val_dataset.sample_idx_path=data/processed/241213-encode_wgbs-train_0_9_val_0_1-ind_tissue/sample_tissue_count_with_idx.csv
--data.val_dataset.num_nbase=1000
--data.val_dataset.gene_expr_quantization=True
--data.val_dataset.is_sequence_tokenized=True
# --data.val_dataset.num_gene_expr_bins=51
# --data.val_dataset.dna_tokenizer_name=zhihan1996/DNABERT-2-117M
